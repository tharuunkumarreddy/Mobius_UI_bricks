name: Load and Process UI Components Dataset
description: Downloads the UI components dataset, processes it according to the new directory structure, and splits it into train/val/test sets.
outputs:
  - {name: train_data, type: Dataset, description: "Pickled dictionary with info for the training set."}
  - {name: val_data, type: Dataset, description: "Pickled dictionary with info for the validation set."}
  - {name: test_data, type: Dataset, description: "Pickled dictionary with info for the test set."}
  - {name: dataset_manifest, type: Dataset, description: "A CSV file manifest of all images and their splits."}
  - {name: categories_json, type: Dataset, description: "A JSON file listing all class categories."}
  - {name: components_mapping, type: Dataset, description: "A JSON file mapping categories to their components."}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        # Install necessary packages
        apt-get update && apt-get install -y unzip
        pip install pandas numpy kaggle

        # Create a directory to work in
        WORKDIR="/ui_components_temp"
        mkdir -p $WORKDIR
        cd $WORKDIR

        echo "Downloading and unzipping the dataset..."
        # This assumes KAGGLE_USERNAME and KAGGLE_KEY are set as environment variables/secrets in Kubeflow
        kaggle datasets download -d chtharunkumarreddy09/classification-dataset-4 -p . --unzip

        echo "Dataset unzipped. Contents of working directory:"
        ls -R .

        # Pass control to the python script
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import random
        import shutil
        import os
        import pickle
        from pathlib import Path
        from math import floor
        import pandas as pd

        # 1. --- Argument Parser ---
        # This defines the output paths that Kubeflow provides.
        parser = argparse.ArgumentParser()
        parser.add_argument('--train_data', type=str, required=True)
        parser.add_argument('--test_data', type=str, required=True)
        parser.add_argument('--val_data', type=str, required=True)
        parser.add_argument('--dataset_manifest', type=str, required=True)
        parser.add_argument('--categories_json', type=str, required=True)
        parser.add_argument('--components_mapping', type=str, required=True)
        args = parser.parse_args()

        # --- Utility Functions ---
        ALLOWED_EXTS = {".jpg", ".jpeg", ".png", ".webp"}

        def list_images(folder: Path):
            return [p for p in folder.iterdir() if p.is_file() and p.suffix.lower() in ALLOWED_EXTS]

        def transfer(src: Path, dst_dir: Path):
            dst_dir.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst_dir / src.name)

        def safe_split(n, tr, va, te):
            base_tr = floor(n * tr)
            base_va = floor(n * va)
            base_te = floor(n * te)
            leftover = n - (base_tr + base_va + base_te)
            counts = {"train": base_tr, "val": base_va, "test": base_te}
            # Distribute remainder to largest splits first
            for split in ["train", "val", "test"]:
                if leftover > 0:
                    counts[split] += 1
                    leftover -= 1
            return counts["train"], counts["val"], counts["test"]

        # 2. --- Main Processing Logic ---
        # **FIXED**: Point to the correct unzipped directory name
        src_root = Path("/ui_components_temp/classification-dataset-4")
        out_root = Path("/tmp/processed_data")
        train_ratio, val_ratio, test_ratio = 0.70, 0.15, 0.15
        random.seed(42)

        if out_root.exists():
            shutil.rmtree(out_root)
        out_root.mkdir(parents=True)
        
        if not src_root.exists():
            raise SystemExit(f"FATAL: Source dataset directory not found at {src_root}. Please check the unzip step.")

        # The new structure is simpler: the directories directly under src_root are the categories.
        categories = sorted([d for d in src_root.iterdir() if d.is_dir()])
        if not categories:
            raise SystemExit(f"FATAL: No category folders found under: {src_root}")

        print(f"Found {len(categories)} categories: {[c.name for c in categories]}")

        # --- File Splitting ---
        for cat_dir in categories:
            components = sorted([d for d in cat_dir.iterdir() if d.is_dir()])
            if not components:
                print(f"[WARN] No component folders in category: {cat_dir.name}")
                continue

            for comp_dir in components:
                imgs = list_images(comp_dir)
                if not imgs:
                    continue

                random.shuffle(imgs)
                n = len(imgs)
                n_tr, n_va, n_te = safe_split(n, train_ratio, val_ratio, test_ratio)

                # Assign files to splits
                train_files = imgs[:n_tr]
                val_files   = imgs[n_tr : n_tr + n_va]
                test_files  = imgs[n_tr + n_va :]

                # Copy files to new structured output directories
                for f in train_files:
                    transfer(f, out_root / "train" / cat_dir.name / comp_dir.name)
                for f in val_files:
                    transfer(f, out_root / "val" / cat_dir.name / comp_dir.name)
                for f in test_files:
                    transfer(f, out_root / "test" / cat_dir.name / comp_dir.name)
        
        print("File splitting and copying complete.")

        # 3. --- Generate Metadata Artifacts ---
        # Create a manifest CSV of all copied files
        rows = []
        for split in ["train", "val", "test"]:
            split_dir = out_root / split
            if not split_dir.exists(): continue
            for cat_dir in sorted(split_dir.iterdir()):
                if not cat_dir.is_dir(): continue
                for comp_dir in sorted(cat_dir.iterdir()):
                    if not comp_dir.is_dir(): continue
                    for img_file in comp_dir.iterdir():
                        rows.append({
                            "path": str(img_file), "split": split,
                            "category": cat_dir.name, "component": comp_dir.name
                        })
        
        manifest_df = pd.DataFrame(rows)
        manifest_path = out_root / "dataset_manifest.csv"
        manifest_df.to_csv(manifest_path, index=False)
        print(f"Manifest created with {len(manifest_df)} entries.")

        # Create categories.json from the directories found in the train set
        train_path = out_root / "train"
        category_names = sorted([d.name for d in train_path.iterdir() if d.is_dir()]) if train_path.exists() else []
        categories_path = out_root / "categories.json"
        with open(categories_path, 'w') as f:
            json.dump(category_names, f, indent=2)

        # Create components_mapping.json
        components_map = {cat_name: sorted([d.name for d in (train_path / cat_name).iterdir() if d.is_dir()]) for cat_name in category_names}
        components_path = out_root / "components_mapping.json"
        with open(components_path, 'w') as f:
            json.dump(components_map, f, indent=2)
            
        # 4. --- Save Outputs for Kubeflow ---
        # For metadata files, we copy the generated file to the Kubeflow output path.
        # This ensures they are plain text (JSON/CSV) as expected by downstream components.
        shutil.copy2(manifest_path, args.dataset_manifest)
        shutil.copy2(categories_path, args.categories_json)
        shutil.copy2(components_path, args.components_mapping)

        # For dataset pointers, we pickle a dictionary containing the path.
        for split in ["train", "val", "test"]:
            output_arg = getattr(args, f"{split}_data")
            data_dict = {
                "data_path": str(out_root / split),
                "split": split
            }
            with open(output_arg, "wb") as f:
                pickle.dump(data_dict, f)
        
        print("All output artifacts have been successfully created.")

    args:
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --val_data
      - {outputPath: val_data}
      - --dataset_manifest
      - {outputPath: dataset_manifest}
      - --categories_json
      - {outputPath: categories_json}
      - --components_mapping
      - {outputPath: components_mapping}
