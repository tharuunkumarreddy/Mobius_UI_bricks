name: load_ui_components_dataset
description: Load UI components dataset from Kaggle, split into train/val/test, and emit train/test/val descriptors plus manifest and metadata.
outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: val_data, type: Dataset}
  - {name: dataset_manifest, type: Dataset}
  - {name: categories_json, type: Dataset}
  - {name: components_mapping, type: Dataset}
implementation:
  container:
    image: python:3.9
    command:
      - bash
      - -ec
      - |
        set -euo pipefail

        # ---- System & Python deps ----
        apt-get update -y && apt-get install -y unzip wget
        python -m pip install --no-cache-dir --upgrade pip
        python -m pip install --no-cache-dir kaggle pandas

        # ---- Hardcode Kaggle credentials (use secrets in prod) ----
        mkdir -p /root/.kaggle
        cat > /root/.kaggle/kaggle.json <<'EOF'
        {
          "username": "chtharunkumarreddy09",
          "key": "1f063391436fcaa34c9d57c545cbd731"
        }
        EOF
        chmod 600 /root/.kaggle/kaggle.json

        # ---- Kaggle download ----
        WORKDIR="/ui_components_temp"
        mkdir -p "$WORKDIR"

        echo "Downloading Kaggle dataset to $WORKDIR ..."
        echo "Dataset URL:"
        echo "https://www.kaggle.com/datasets/chtharunkumarreddy09/classification-dataset-4"
        kaggle datasets download -d chtharunkumarreddy09/classification-dataset-4 -p "$WORKDIR" --force

        ZIP_PATH="$(ls -1 "$WORKDIR"/*.zip | head -n1 || true)"
        if [ -z "$ZIP_PATH" ]; then
          echo "ERROR: Zip not found after download." >&2
          exit 1
        fi

        echo "Unzipping $ZIP_PATH ..."
        unzip -q "$ZIP_PATH" -d "$WORKDIR"

        # Updated: Detect dataset root - look for the extracted folder directly
        # The dataset now has categories directly under the main folder
        if SRC_DIR="$(find "$WORKDIR" -type d -name 'classification-dataset-4' -print -quit)"; then
          echo "Found classification-dataset-4 directory: $SRC_DIR"
        else
          # Fallback: look for any directory that contains subdirectories (categories)
          SRC_DIR="$(find "$WORKDIR" -mindepth 1 -maxdepth 1 -type d | head -n1 || true)"
          if [ -n "$SRC_DIR" ]; then
            # Check if this directory contains category subdirectories
            if [ "$(find "$SRC_DIR" -mindepth 1 -maxdepth 1 -type d | wc -l)" -gt 0 ]; then
              echo "Using directory with subdirectories: $SRC_DIR"
            else
              echo "ERROR: Found directory but it doesn't contain subdirectories." >&2
              exit 1
            fi
          fi
        fi

        if [ -z "$SRC_DIR" ] || [ ! -d "$SRC_DIR" ]; then
          echo "ERROR: Could not locate extracted dataset directory under $WORKDIR." >&2
          echo "Available directories:" >&2
          find "$WORKDIR" -maxdepth 2 -type d -print >&2 || true
          exit 1
        fi

        echo "Detected dataset root: $SRC_DIR"
        export SRC_DIR

        echo "Forwarded container args (for debugging):" "$@"

        # ---- Run processing script; use environment variables instead of args ----
        # Set environment variables for the Python script
        export TRAIN_DATA_PATH="$1"
        export TEST_DATA_PATH="$3"
        export VAL_DATA_PATH="$5" 
        export DATASET_MANIFEST_PATH="$7"
        export CATEGORIES_JSON_PATH="$9"
        export COMPONENTS_MAPPING_PATH="${11}"

        cat > /tmp/process_dataset.py <<'PY'
        #!/usr/bin/env python3
        import json, random, shutil, os, pickle, sys
        from pathlib import Path
        from math import floor
        import pandas as pd

        # Defaults that match KFP container contract (use env vars instead of args)
        def _kfp_out(name: str) -> str:
            return f"/tmp/outputs/{name}/data"

        # Get paths from environment variables (set by bash script)
        train_data_path = os.environ.get('TRAIN_DATA_PATH', _kfp_out("train_data"))
        test_data_path = os.environ.get('TEST_DATA_PATH', _kfp_out("test_data"))
        val_data_path = os.environ.get('VAL_DATA_PATH', _kfp_out("val_data"))
        dataset_manifest_path = os.environ.get('DATASET_MANIFEST_PATH', _kfp_out("dataset_manifest"))
        categories_json_path = os.environ.get('CATEGORIES_JSON_PATH', _kfp_out("categories_json"))
        components_mapping_path = os.environ.get('COMPONENTS_MAPPING_PATH', _kfp_out("components_mapping"))

        ALLOWED_EXTS = {".jpg", ".jpeg", ".png", ".webp"}

        def list_images(folder: Path):
            return [p for p in folder.iterdir() if p.is_file() and p.suffix.lower() in ALLOWED_EXTS]

        def ensure_dirs(p: Path):
            p.mkdir(parents=True, exist_ok=True)

        def transfer(src: Path, dst_dir: Path, move: bool):
            ensure_dirs(dst_dir)
            if move:
                shutil.move(str(src), str(dst_dir / src.name))
            else:
                shutil.copy2(src, dst_dir / src.name)

        def is_category_dir(d: Path):
            """Check if directory contains component subdirectories"""
            return d.is_dir() and any(child.is_dir() for child in d.iterdir())

        def get_categories(root: Path):
            """Get category directories from the root path"""
            print(f"[DEBUG] Looking for categories in: {root}")
            
            # List all directories in root
            all_dirs = [d for d in root.iterdir() if d.is_dir()]
            print(f"[DEBUG] Found {len(all_dirs)} directories in root:")
            for d in all_dirs:
                print(f"  - {d.name}")
            
            # Filter directories that contain component subdirectories
            category_dirs = [d for d in all_dirs if is_category_dir(d)]
            print(f"[DEBUG] Found {len(category_dirs)} category directories:")
            for d in category_dirs:
                components = [c for c in d.iterdir() if c.is_dir()]
                print(f"  - {d.name} ({len(components)} components)")
            
            return category_dirs

        def safe_split(n, tr, va, te):
            base_tr = floor(n * tr)
            base_va = floor(n * va)
            base_te = floor(n * te)
            leftover = n - (base_tr + base_va + base_te)
            parts = [("train", tr), ("val", va), ("test", te)]
            parts.sort(key=lambda x: x[1], reverse=True)
            counts = {"train": base_tr, "val": base_va, "test": base_te}
            for name, _ in parts:
                if leftover <= 0: break
                counts[name] += 1
                leftover -= 1
            return counts["train"], counts["val"], counts["test"]

        def discover_components(train_root: Path):
            mapping = {}
            if not train_root.exists():
                return mapping
            for cat in sorted([d for d in train_root.iterdir() if d.is_dir()]):
                comps = [x.name for x in sorted(cat.iterdir()) if x.is_dir()]
                mapping[cat.name] = comps
            return mapping

        # Source dataset root from env (set by shell after unzip)
        src_root_env = os.environ.get("SRC_DIR", "").strip()
        if not src_root_env:
            # Fallback to common locations
            potential_paths = [
                Path("/ui_components_temp/classification-dataset-4"),
                Path("/ui_components_temp")
            ]
            for p in potential_paths:
                if p.exists() and any(d.is_dir() for d in p.iterdir()):
                    src_root = p
                    break
            else:
                raise SystemExit("Could not find dataset root directory")
        else:
            src_root = Path(src_root_env)
            
        print(f"[INFO] Using source root: {src_root}")

        out_root = Path("/tmp/processed_data")
        train_ratio, val_ratio, test_ratio = 0.70, 0.15, 0.15
        seed, move_files = 42, False

        assert abs((train_ratio + val_ratio + test_ratio) - 1.0) < 1e-6, "Ratios must sum to 1.0"
        random.seed(seed)
        ensure_dirs(out_root)

        if not src_root.exists():
            raise SystemExit(f"Source root does not exist: {src_root}")

        categories = get_categories(src_root)
        if not categories:
            print(f"[ERROR] No category folders found under: {src_root}")
            print("[ERROR] Directory contents:")
            if src_root.exists():
                for p in src_root.iterdir():
                    print(f"  - {p.name} ({'directory' if p.is_dir() else 'file'})")
            raise SystemExit(f"No category folders found under: {src_root}")

        total_images = 0
        for cat in categories:
            print(f"[INFO] Processing category: {cat.name}")
            components = [d for d in cat.iterdir() if d.is_dir()]
            if not components:
                print(f"[WARN] No component folders in category: {cat.name}")
                continue

            for comp in components:
                imgs = list_images(comp)
                if not imgs:
                    print(f"[WARN] No images in: {cat.name}/{comp.name}")
                    continue

                print(f"[INFO] Processing {cat.name}/{comp.name}: {len(imgs)} images")
                total_images += len(imgs)

                random.shuffle(imgs)
                n = len(imgs)
                n_tr, n_va, n_te = safe_split(n, train_ratio, val_ratio, test_ratio)

                train_files = imgs[:n_tr]
                val_files   = imgs[n_tr:n_tr+n_va]
                test_files  = imgs[n_tr+n_va:n_tr+n_va+n_te]

                for f in train_files:
                    transfer(f, out_root / "train" / cat.name / comp.name, move=move_files)
                for f in val_files:
                    transfer(f, out_root / "val" / cat.name / comp.name, move=move_files)
                for f in test_files:
                    transfer(f, out_root / "test" / cat.name / comp.name, move=move_files)

        print(f"[INFO] Total images processed: {total_images}")

        # Create manifest
        rows = []
        for split in ["train", "val", "test"]:
            split_dir = out_root / split
            if not split_dir.exists(): 
                continue
            for cat in sorted([d for d in split_dir.iterdir() if d.is_dir()]):
                for comp in sorted([c for c in cat.iterdir() if c.is_dir()]):
                    for img in comp.iterdir():
                        if img.suffix.lower() in ALLOWED_EXTS:
                            rows.append({
                                "path": str(img), 
                                "split": split, 
                                "category": cat.name, 
                                "component": comp.name
                            })

        manifest = pd.DataFrame(rows)
        manifest_path = out_root / "dataset_manifest.csv"
        manifest.to_csv(manifest_path, index=False)
        print(f"[INFO] Created manifest with {len(manifest)} entries")

        # Create categories list
        cats = sorted([d.name for d in (out_root / "train").iterdir() if d.is_dir()]) if (out_root / "train").exists() else []
        categories_path = out_root / "categories.json"
        categories_path.write_text(json.dumps(cats, indent=2))
        print(f"[INFO] Found {len(cats)} categories: {cats}")

        # Create components mapping
        comp_map = discover_components(out_root / "train")
        components_path = out_root / "components_mapping.json"
        components_path.write_text(json.dumps(comp_map, indent=2))
        print(f"[INFO] Created components mapping for {len(comp_map)} categories")

        def _makedirs_for(path_str: str) -> Path:
            p = Path(path_str)
            p.parent.mkdir(parents=True, exist_ok=True)
            return p

        # Save pickle files for KFP outputs
        with open(_makedirs_for(train_data_path), "wb") as f:
            pickle.dump({
                "data_path": str(out_root / "train"),
                "manifest_path": str(manifest_path),
                "categories": cats,
                "components_mapping": comp_map,
                "split": "train"
            }, f)

        with open(_makedirs_for(test_data_path), "wb") as f:
            pickle.dump({
                "data_path": str(out_root / "test"),
                "manifest_path": str(manifest_path),
                "categories": cats,
                "components_mapping": comp_map,
                "split": "test"
            }, f)

        with open(_makedirs_for(val_data_path), "wb") as f:
            pickle.dump({
                "data_path": str(out_root / "val"),
                "manifest_path": str(manifest_path),
                "categories": cats,
                "components_mapping": comp_map,
                "split": "val"
            }, f)

        with open(_makedirs_for(dataset_manifest_path), "wb") as f:
            pickle.dump(manifest, f)

        with open(_makedirs_for(categories_json_path), "wb") as f:
            pickle.dump(cats, f)

        with open(_makedirs_for(components_mapping_path), "wb") as f:
            pickle.dump(comp_map, f)

        print(f"[INFO] Output tree root: {out_root}")
        print(f"[INFO] Manifest saved to: {manifest_path}")
        print(f"[INFO] Categories saved to: {categories_path}")
        print(f"[INFO] Components mapping saved to: {components_path}")
        print("[SUCCESS] Successfully processed and saved UI Components dataset.")
        PY

        # Execute the Python script (no arguments needed now)
        python /tmp/process_dataset.py
    args:
      - --train_data
      - {outputPath: train_data}
      - --test_data
      - {outputPath: test_data}
      - --val_data
      - {outputPath: val_data}
      - --dataset_manifest
      - {outputPath: dataset_manifest}
      - --categories_json
      - {outputPath: categories_json}
      - --components_mapping
      - {outputPath: components_mapping}
