name: load_ui_components_dataset
description: Load UI components dataset from Kaggle, split into train/val/test, and emit train/test/val folders plus manifest and metadata.
outputs:
  - { name: train_data, type: Dataset, description: "Directory: train split (category/component/IMGs)" }
  - { name: test_data,  type: Dataset, description: "Directory: test split (category/component/IMGs)" }
  - { name: val_data,   type: Dataset, description: "Directory: val split (category/component/IMGs)" }
  - { name: dataset_manifest,   type: Dataset, description: "CSV manifest of all splits" }
  - { name: categories_json,    type: Dataset, description: "JSON list of category names" }
  - { name: components_mapping, type: Dataset, description: "JSON dict: {category: [components...]}" }
implementation:
  container:
    image: python:3.9
    command:
      - bash
      - -ec
      - |
        set -euo pipefail

        # ---------- System & Python deps (consider pre-baking) ----------
        apt-get update -y && apt-get install -y unzip wget
        python -m pip install --no-cache-dir --upgrade pip
        python -m pip install --no-cache-dir kaggle pandas

        # ---------- Hardcode Kaggle credentials ----------
        mkdir -p /root/.kaggle
        cat > /root/.kaggle/kaggle.json <<'EOF'
        {
          "username": "chtharunkumarreddy09",
          "key": "1f063391436fcaa34c9d57c545cbd731"
        }
        EOF
        chmod 600 /root/.kaggle/kaggle.json
        # ---------- Download dataset ----------
        WORKDIR="/ui_components_temp"
        mkdir -p "$WORKDIR"
        kaggle datasets download -d chtharunkumarreddy09/Minor_Classification_Data -p "$WORKDIR" --force

        ZIP_PATH="$(ls -1 "$WORKDIR"/*.zip | head -n1 || true)"
        [ -n "$ZIP_PATH" ] || { echo "ERROR: Zip not found after download"; exit 1; }
        unzip -q "$ZIP_PATH" -d "$WORKDIR"

        # Detect root folder of extracted dataset
        SRC_DIR="$(find "$WORKDIR" -type d -name 'minor_Classification_Dataset' -print -quit || true)"
        [ -n "$SRC_DIR" ] || SRC_DIR="$(find "$WORKDIR" -mindepth 1 -maxdepth 1 -type d | head -n1 || true)"
        [ -d "$SRC_DIR" ] || { echo "ERROR: Could not locate dataset root under $WORKDIR"; exit 1; }
        export SRC_DIR
        echo "Detected dataset root: $SRC_DIR"

        # ---------- Prepare processor ----------
        cat > /tmp/process_dataset.py <<'PY'
        #!/usr/bin/env python3
        import json, random, shutil, os, sys
        from pathlib import Path
        from math import floor
        import argparse
        import pandas as pd

        ALLOWED_EXTS = {".jpg", ".jpeg", ".png", ".webp"}

        def list_images(folder: Path):
            return [p for p in folder.iterdir() if p.is_file() and p.suffix.lower() in ALLOWED_EXTS]

        def is_category_dir(d: Path):
            # category contains component subfolders
            return d.is_dir() and any(child.is_dir() for child in d.iterdir())

        def get_categories(root: Path):
            cats = [d for d in root.iterdir() if is_category_dir(d)]
            if not cats:
                # Try one more level down
                for d in [d for d in root.iterdir() if d.is_dir()]:
                    deeper = [x for x in d.iterdir() if is_category_dir(x)]
                    if deeper:
                        return deeper
            return cats

        def safe_split(n, tr, va, te):
            base_tr = int(n * tr)
            base_va = int(n * va)
            base_te = int(n * te)
            leftover = n - (base_tr + base_va + base_te)
            parts = [("train", tr), ("val", va), ("test", te)]
            parts.sort(key=lambda x: x[1], reverse=True)
            counts = {"train": base_tr, "val": base_va, "test": base_te}
            for name, _ in parts:
                if leftover <= 0: break
                counts[name] += 1
                leftover -= 1
            return counts["train"], counts["val"], counts["test"]

        def copyfile(src: Path, dst: Path):
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)

        def discover_components(train_root: Path):
            mapping = {}
            if not train_root.exists():
                return mapping
            for cat in sorted([d for d in train_root.iterdir() if d.is_dir()]):
                comps = [x.name for x in sorted(cat.iterdir()) if x.is_dir()]
                mapping[cat.name] = comps
            return mapping

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--src_root", required=True, help="Extracted Kaggle dataset root")
            ap.add_argument("--out_train", required=True)    # directory
            ap.add_argument("--out_val", required=True)      # directory
            ap.add_argument("--out_test", required=True)     # directory
            ap.add_argument("--out_manifest", required=True) # file (CSV)
            ap.add_argument("--out_categories", required=True) # file (JSON)
            ap.add_argument("--out_components", required=True) # file (JSON)
            ap.add_argument("--train_ratio", type=float, default=0.70)
            ap.add_argument("--val_ratio", type=float, default=0.15)
            ap.add_argument("--test_ratio", type=float, default=0.15)
            ap.add_argument("--seed", type=int, default=42)
            args = ap.parse_args()

            assert abs((args.train_ratio + args.val_ratio + args.test_ratio) - 1.0) < 1e-6

            src_root = Path(args.src_root)
            if not src_root.exists():
                raise SystemExit(f"Source root not found: {src_root}")

            random.seed(args.seed)

            out_train = Path(args.out_train)
            out_val   = Path(args.out_val)
            out_test  = Path(args.out_test)
            out_train.mkdir(parents=True, exist_ok=True)
            out_val.mkdir(parents=True, exist_ok=True)
            out_test.mkdir(parents=True, exist_ok=True)

            categories = get_categories(src_root)
            if not categories:
                raise SystemExit(f"No category folders found under: {src_root}")

            # Split & copy
            for cat in categories:
                components = [d for d in cat.iterdir() if d.is_dir()]
                if not components:
                    print(f"[WARN] No components in category: {cat.name}")
                    continue

                for comp in components:
                    imgs = list_images(comp)
                    if not imgs:
                        print(f"[WARN] No images in: {cat.name}/{comp.name}")
                        continue

                    random.shuffle(imgs)
                    n_tr, n_va, n_te = safe_split(len(imgs), args.train_ratio, args.val_ratio, args.test_ratio)
                    train_files = imgs[:n_tr]
                    val_files   = imgs[n_tr:n_tr+n_va]
                    test_files  = imgs[n_tr+n_va:n_tr+n_va+n_te]

                    for f in train_files:
                        dst = out_train / cat.name / comp.name / f.name
                        copyfile(f, dst)
                    for f in val_files:
                        dst = out_val / cat.name / comp.name / f.name
                        copyfile(f, dst)
                    for f in test_files:
                        dst = out_test / cat.name / comp.name / f.name
                        copyfile(f, dst)

            # Manifest
            rows = []
            for split, root in [("train", out_train), ("val", out_val), ("test", out_test)]:
                if not root.exists(): continue
                for cat in sorted([d for d in root.iterdir() if d.is_dir()]):
                    for comp in sorted([c for c in cat.iterdir() if c.is_dir()]):
                        for img in comp.rglob("*"):
                            if img.is_file() and img.suffix.lower() in ALLOWED_EXTS:
                                rows.append({
                                    "path": str(img),
                                    "split": split,
                                    "category": cat.name,
                                    "component": comp.name
                                })
            manifest = pd.DataFrame(rows)
            Path(args.out_manifest).parent.mkdir(parents=True, exist_ok=True)
            manifest.to_csv(args.out_manifest, index=False)

            # Categories & components mapping based on the *train* split
            cats = sorted([d.name for d in out_train.iterdir() if d.is_dir()]) if out_train.exists() else []
            comp_map = discover_components(out_train)

            Path(args.out_categories).parent.mkdir(parents=True, exist_ok=True)
            Path(args.out_components).parent.mkdir(parents=True, exist_ok=True)
            Path(args.out_categories).write_text(json.dumps(cats, indent=2))
            Path(args.out_components).write_text(json.dumps(comp_map, indent=2))

            print("Emitted artifacts:")
            print(" - train:", out_train)
            print(" - val  :", out_val)
            print(" - test :", out_test)
            print(" - manifest:", args.out_manifest)
            print(" - categories:", args.out_categories)
            print(" - components:", args.out_components)

        if __name__ == "__main__":
            main()
        PY

        # ---------- Run processor with explicit output paths ----------
        python /tmp/process_dataset.py \
          --src_root "${SRC_DIR}" \
          --out_train        "$1" \
          --out_val          "$5" \
          --out_test         "$3" \
          --out_manifest     "$7" \
          --out_categories   "$9" \
          --out_components   "${11}"
    args:
      - --train_data
      - {outputPath: train_data}          # directory artifact
      - --test_data
      - {outputPath: test_data}           # directory artifact
      - --val_data
      - {outputPath: val_data}            # directory artifact
      - --dataset_manifest
      - {outputPath: dataset_manifest}    # file (CSV)
      - --categories_json
      - {outputPath: categories_json}     # file (JSON)
      - --components_mapping
      - {outputPath: components_mapping}  # file (JSON)
