name: probir_Finetune_LoRA_Model_1644
description: Tokenizes corpus to memmaps and trains Gemma3 with AMP, warmup→cosine LR, gradient accumulation, and best/final checkpoints.

inputs:
  # Dataset and Model inputs
  - name: tokenizer_json
    type: Model
  - name: train_corpus
    type: Data
  - name: model_config
    type: Data
  - name: model_weights
    type: Model
  - name: model_py_in
    type: Data

  # Hyperparameters (same defaults)
  - name: learning_rate
    type: String
    default: "1e-4"
  - name: min_lr
    type: String
    default: "1e-5"
  - name: warmup_steps
    type: Integer
    default: "1000"
  - name: max_iters
    type: Integer
    default: "150000"
  - name: batch_size
    type: Integer
    default: "32"
  - name: block_size
    type: Integer
    default: "128"
  - name: grad_accum
    type: Integer
    default: "32"
  - name: eval_interval
    type: Integer
    default: "1000"
  - name: eval_iters
    type: Integer
    default: "500"
  - name: weight_decay
    type: String
    default: "0.1"
  - name: beta2
    type: String
    default: "0.95"
  - name: clip_grad_norm
    type: String
    default: "0.5"
  - name: val_fraction
    type: String
    default: "0.1"
  - name: num_proc
    type: Integer
    default: "8"

  # LoRA-specific hyperparameters
  - name: r
    type: Integer
    default: "8"
  - name: lora_alpha
    type: Integer
    default: "16"
  - name: lora_dropout
    type: Float
    default: "0.05"
  - name: bias
    type: String
    default: "none"
  - name: lora_target_modules
    type: List
    default: []

outputs:
  - name: best_weights
    type: Model
  - name: final_weights
    type: Model
  - name: training_report
    type: Data
  - name: loss_curve_csv
    type: Data
  - name: model_py
    type: Data
  - name: schema_json
    type: String   # String so downstream receives the JSON text

implementation:
  container:
    image: python:3.9
    command:
      - python3
      - -u
      - -c
      - |-
        import subprocess, sys
        subprocess.run([sys.executable, "-m", "pip", "install", "--quiet", "--no-cache-dir", 
                        "torch>=2.2", "tokenizers", "datasets", "tqdm", "numpy", "peft"], check=True)

        # Importing the finetuning logic
        import os, json, math, shutil, random, importlib.util
        import numpy as np
        import torch
        from tokenizers import Tokenizer
        from datasets import load_dataset
        from tqdm import tqdm
        from peft import LoraConfig, get_peft_model, get_peft_model_state_dict

        # Finetuning logic - directly copying the finetuning.py logic
        def seed_everything(seed: int = 42):
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False

        def _find_model_py_file(path_or_dir: str) -> str:
            if os.path.isfile(path_or_dir):
                return path_or_dir
            if not os.path.isdir(path_or_dir):
                raise ValueError(f"Path {path_or_dir} is neither a file nor a directory")
            py_files = [os.path.join(path_or_dir, f) for f in os.listdir(path_or_dir) if f.endswith(".py")]
            if not py_files:
                raise FileNotFoundError(f"No .py file found in directory {path_or_dir}")
            return py_files[0]

        def _load_model_class_from_file(py_path: str, class_name: str = "Gemma3Model"):
            if not py_path.endswith(".py"):
                tmp = py_path + ".py"
                shutil.copyfile(py_path, tmp)
                py_path = tmp
            spec = importlib.util.spec_from_file_location("gemma3_model", py_path)
            if spec is None or spec.loader is None:
                raise ImportError(f"Could not load spec from {py_path}")
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            if not hasattr(mod, class_name):
                raise AttributeError(f"{py_path} does not define {class_name}")
            return getattr(mod, class_name)

        def _choose_bin_dtype(vocab_size: int):
            if vocab_size <= (1 << 16): return np.uint16
            if vocab_size <= (1 << 32): return np.uint32
            return np.uint64

        def _tokenize_to_bins(tokenizer_path: str, train_txt: str, val_fraction: float, num_proc: int) -> Dict[str, Any]:
            tok = Tokenizer.from_file(tokenizer_path)
            vocab_size = tok.get_vocab_size()
            BIN_DTYPE = _choose_bin_dtype(vocab_size)

            ds_single = load_dataset("text", data_files={"train": train_txt})
            split = ds_single["train"].train_test_split(test_size=float(val_fraction), seed=42)
            ds = {"train": split["train"], "val": split["test"]}

            def proc(ex):
                ids = tok.encode(ex["text"]).ids
                return {"ids": ids, "len": len(ids)}

            nproc = max(1, min(int(num_proc), (os.cpu_count() or 1)))
            tokenized = {sp: d.map(proc, remove_columns=d.column_names, desc=f"tokenizing {sp}", num_proc=nproc)
                         for sp, d in ds.items()}

            def write_split(dset, filename):
                total = int(np.sum(dset["len"], dtype=np.uint64))
                arr = np.memmap(filename, dtype=BIN_DTYPE, mode="w+", shape=(total,))
                shard_size = max(1, len(dset) // 1024)
                total_shards = max(1, (len(dset) + shard_size - 1) // shard_size)
                idx = 0
                for i in tqdm(range(total_shards), desc=f"writing {filename}"):
                    start, stop = i * shard_size, min(len(dset), (i + 1) * shard_size)
                    if start >= stop: continue
                    batch = dset.select(range(start, stop)).with_format(type="numpy")
                    ids_list = batch["ids"]
                    arr_batch = np.concatenate([np.asarray(x, dtype=BIN_DTYPE) for x in ids_list])
                    arr[idx: idx + len(arr_batch)] = arr_batch
                    idx += len(arr_batch)
                arr.flush()
                return total

            train_tokens = write_split(tokenized["train"], "train.bin")
            val_tokens   = write_split(tokenized["val"], "val.bin")

            meta = {"vocab_size": vocab_size, "train_tokens": int(train_tokens), "val_tokens": int(val_tokens),
                    "bin_dtype": str(BIN_DTYPE.__name__)}
            with open("bin_meta.json", "w", encoding="utf-8") as f: json.dump(meta, f, indent=2)
            return meta

        def _scheduler_warmup_cosine(optimizer, total_updates: int, warmup_steps: int, min_lr: float):
            from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR

            total_updates = max(1, int(total_updates))
            warm_updates  = max(0, min(int(warmup_steps), total_updates))

            base_lr = optimizer.param_groups[0]["lr"]
            eps = 1e-8
            start_factor = max((min_lr / base_lr) if base_lr > 0 else 0.0, eps)
            start_factor = min(start_factor, 1.0)

            if warm_updates > 0 and start_factor < 1.0:
                sched_warm = LinearLR(optimizer, total_iters=warm_updates, start_factor=start_factor)
                remain = max(1, total_updates - warm_updates)
                sched_cos  = CosineAnnealingLR(optimizer, T_max=remain, eta_min=min_lr)
                scheduler  = SequentialLR(optimizer, schedulers=[sched_warm, sched_cos], milestones=[warm_updates])
            else:
                scheduler  = CosineAnnealingLR(optimizer, T_max=total_updates, eta_min=min_lr)

            return scheduler, warm_updates

        def finetune_lora(
            tokenizer_json, train_corpus, model_config, model_weights, model_py_in, model_py_out,
            learning_rate, min_lr, warmup_steps, max_iters, batch_size, block_size, grad_accum, eval_interval, eval_iters,
            weight_decay, beta2, clip_grad_norm, val_fraction, num_proc, r, lora_alpha, lora_dropout, bias, lora_target_modules,
            best_weights, final_weights, training_report, loss_curve_csv, schema_output):

            seed_everything(42)

            device = "cuda" if torch.cuda.is_available() else "cpu"
            device_type = "cuda" if device == "cuda" else "cpu"
            print(f"[INFO] Finetuning (LoRA) on {device.upper()}")
            ctx, scaler, _dtype_name = autocast_context(device_type)

            model_py_path = _find_model_py_file(model_py_in)
            os.makedirs(os.path.dirname(model_py_out) or ".", exist_ok=True)
            shutil.copyfile(model_py_path, model_py_out)

            with open(model_config, "r", encoding="utf-8") as f:
                cfg = json.load(f)
            cfg["dtype"] = torch.float16 if str(cfg.get("dtype")) == "float16" else torch.float32

            _validate_hparams(learning_rate, min_lr)

            tok = Tokenizer.from_file(tokenizer_json)
            Gemma3Model = _load_model_class_from_file(model_py_path, "Gemma3Model")
            base_model = Gemma3Model(cfg).to(device)
            state = torch.load(model_weights, map_location=device)
            base_model.load_state_dict(state, strict=True)
            base_model.train()

            wrapped = HFCompatCausalLM(base_model)

            auto_targets = _infer_lora_targets(wrapped, user_list=lora_target_modules)
            peft_cfg = LoraConfig(
                r=r,
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
                bias=bias,
                target_modules=auto_targets,
                task_type="FEATURE_EXTRACTION",
            )
            model = get_peft_model(wrapped, peft_cfg)
            model.print_trainable_parameters()

            meta = _tokenize_to_bins(tokenizer_json, train_corpus, val_fraction, num_proc)
            batcher = _Batcher(device, device_type, block_size, batch_size)
            get_batch = batcher.get_batch

            optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, beta2), weight_decay=weight_decay, eps=1e-9)
            total_updates = math.ceil(max_iters / max(1, grad_accum))
            scheduler, warm_updates = _scheduler_warmup_cosine(optimizer, total_updates, warmup_steps, min_lr)

            best_val = float("inf")
            did_best_save = False
            train_curve, val_curve, lr_curve = [], [], []
            updates = 0

            try:
                init_losses = _estimate_loss(model, get_batch, eval_iters, ctx)
                best_val = init_losses["val"]
                _save_peft_adapters(model, best_weights)
                did_best_save = True
                print(f"[INFO] Initial eval → val={best_val:.4f}, saved as best.")
            except Exception as e:
                print(f"[WARN] Initial evaluation skipped ({e}).")

            pbar = tqdm(range(max_iters), desc="finetune (LoRA) micro-steps")
            optimizer.zero_grad(set_to_none=True)
            for step in pbar:
                X, Y = get_batch("train")
                with ctx:
                    _, loss = model(input_ids=X, labels=Y)

                loss = loss / grad_accum
                if scaler.is_enabled():
                    scaler.scale(loss).backward()
                else:
                    loss.backward()

                if ((step + 1) % grad_accum == 0) or (step + 1 == max_iters):
                    if scaler.is_enabled():
                        scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)
                    if scaler.is_enabled():
                        scaler.step(optimizer); scaler.update()
                    else:
                        optimizer.step()
                    optimizer.zero_grad(set_to_none=True)
                    scheduler.step()
                    updates += 1

                    if updates % max(1, eval_interval) == 0:
                        losses = _estimate_loss(model, get_batch, eval_iters, ctx)
                        train_curve.append(losses["train"])
                        val_curve.append(losses["val"])
                        lr_curve.append(optimizer.param_groups[0]["lr"])
                        pbar.set_postfix(train=f"{losses['train']:.4f}",
                                         val=f"{losses['val']:.4f}",
                                         lr=f"{optimizer.param_groups[0]['lr']:.2e}")
                        if losses["val"] < best_val:
                            best_val = losses["val"]
                            _save_peft_adapters(model, best_weights)
                            did_best_save = True

            # Final adapters
            _save_peft_adapters(model, final_weights)
            if not did_best_save:
                shutil.copyfile(final_weights, best_weights)

            # 7) artifacts
            report = {
                "best_val_loss": best_val,
                "final_lr": optimizer.param_groups[0]["lr"],
                "updates": updates,
                "max_iters": max_iters,
                "batch_size": batch_size,
                "block_size": block_size,
                "grad_accum": grad_accum,
                "train_tokens": meta.get("train_tokens"),
                "val_tokens": meta.get("val_tokens"),
                "total_updates": total_updates,
                "warmup_updates": warm_updates,
                "total_trainable_params": trainable_params,
                "note": "Weights saved are LoRA adapter weights only (compatible with PEFT).",
            }
            os.makedirs(os.path.dirname(training_report) or ".", exist_ok=True)
            with open(training_report, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2)

            os.makedirs(os.path.dirname(loss_curve_csv) or ".", exist_ok=True)
            with open(loss_curve_csv, "w", newline="", encoding="utf-8") as f:
                w = csv.writer(f); w.writerow(["update","train_loss","val_loss","lr"])
                for i,(tr,va,lr) in enumerate(zip(train_curve, val_curve, lr_curve), start=1):
                    w.writerow([i, tr, va, lr])

            if schema_output:
                schema_list = [
                    {"epoch": int(i), "loss": float(tr), "validation_loss": float(va)}
                    for i, (tr, va) in enumerate(zip(train_curve, val_curve), start=1)
                ]
                os.makedirs(os.path.dirname(schema_output) or ".", exist_ok=True)
                with open(schema_output, "w", encoding="utf-8") as f:
                    json.dump(schema_list, f, separators=(",", ":"))

            print("[DONE] LoRA finetuning complete. best:", best_val, "final adapters saved:", final_weights)
            return report

        if __name__ == "__main__":
            main()
    args:
      - --tokenizer-json
      - {inputPath: tokenizer_json}
      - --train-corpus
      - {inputPath: train_corpus}
      - --model-config
      - {inputPath: model_config}
      - --model-weights
      - {inputPath: model_weights}
      - --model-py-in
      - {inputPath: model_py_in}

      # Hyperparameters
      - --learning-rate
      - {inputValue: learning_rate}
      - --min-lr
      - {inputValue: min_lr}
      - --warmup-steps
      - {inputValue: warmup_steps}
      - --max-iters
      - {inputValue: max_iters}
      - --batch-size
      - {inputValue: batch_size}
      - --block-size
      - {inputValue: block_size}
      - --grad-accum
      - {inputValue: grad_accum}
      - --eval-interval
      - {inputValue: eval_interval}
      - --eval-iters
      - {inputValue: eval_iters}
      - --weight-decay
      - {inputValue: weight_decay}
      - --beta2
      - {inputValue: beta2}
      - --clip-grad-norm
      - {inputValue: clip_grad_norm}
      - --val-fraction
      - {inputValue: val_fraction}
      - --num-proc
      - {inputValue: num_proc}

      # LoRA settings
      - --r
      - {inputValue: r}
      - --lora-alpha
      - {inputValue: lora_alpha}
      - --lora-dropout
      - {inputValue: lora_dropout}
      - --bias
      - {inputValue: bias}
      - --lora-target-modules
      - {inputValue: lora_target_modules}

      # Outputs
      - --best-weights
      - {outputPath: best_weights}
      - --final-weights
      - {outputPath: final_weights}
      - --training-report
      - {outputPath: training_report}
      - --loss-curve-csv
      - {outputPath: loss_curve_csv}
      - --schema-output
      - {outputPath: schema_json}
