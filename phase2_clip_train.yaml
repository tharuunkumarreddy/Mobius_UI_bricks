name: phase2_clip_train
description: Train specialized CLIP classification heads per category (Phase 2). 
             Takes dataset splits and phase2_model_config.json, outputs per-category best models and training results.

inputs:
  - { name: train_data,   type: Dataset, description: "Directory: train split (category/component/IMGs)" }
  - { name: val_data,     type: Dataset, description: "Directory: val split (category/component/IMGs)" }
  - { name: test_data,    type: Dataset, description: "Directory: test split (category/component/IMGs)" }
  - { name: model_config, type: Dataset, description: "phase2_model_config.json from previous brick" }

outputs:
  - { name: buttons_actions_model,        type: Model, description: "Best CLIP model for buttons_actions category" }
  - { name: content_display_model,        type: Model, description: "Best CLIP model for content_display category" }
  - { name: feedback_status_model,        type: Model, description: "Best CLIP model for feedback_status category" }
  - { name: input_forms_model,            type: Model, description: "Best CLIP model for input_forms category" }
  - { name: interactive_elements_model,   type: Model, description: "Best CLIP model for interactive_elements category" }
  - { name: layout_structure_model,       type: Model, description: "Best CLIP model for layout_structure category" }
  - { name: navigation_model,             type: Model, description: "Best CLIP model for navigation category" }
  - { name: visual_enhancements_model,    type: Model, description: "Best CLIP model for visual_enhancements category" }
  - { name: training_results,             type: Dataset, description: "phase2_training_results.json with per-category accuracies" }

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        pip install torch torchvision pillow --index-url https://download.pytorch.org/whl/cpu
        pip install git+https://github.com/openai/CLIP.git
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, json
        from pathlib import Path
        from typing import Dict, List
        import torch, torch.nn as nn, torch.optim as optim
        from torch.utils.data import Dataset, DataLoader
        from torchvision import transforms
        from PIL import Image
        import clip

        ALLOWED = {".jpg", ".jpeg", ".png", ".webp"}

        def discover_components_in_category(data_dir: Path, category: str, split: str = "train"):
            category_path = data_dir / split / category
            if not category_path.exists():
                return []
            return sorted([item.name for item in category_path.iterdir() if item.is_dir()])

        def get_all_components_mapping(data_dir: Path, categories: List[str]):
            mapping: Dict[str, List[str]] = {}
            for cat in categories:
                comps = discover_components_in_category(data_dir, cat, "train")
                if comps: mapping[cat] = comps
            return mapping

        class CategoryComponentDataset(Dataset):
            def __init__(self, root: Path, category: str, components: List[str], split: str, transform=None):
                self.transform = transform
                self.component_to_idx = {c: i for i, c in enumerate(components)}
                self.samples = []
                category_folder = root / split / category
                for comp in components:
                    comp_dir = category_folder / comp
                    if comp_dir.exists():
                        for img in comp_dir.iterdir():
                            if img.suffix.lower() in ALLOWED:
                                self.samples.append((img, self.component_to_idx[comp]))
                if not self.samples:
                    raise ValueError(f"No samples for {category} in {split}")

            def __len__(self): return len(self.samples)
            def __getitem__(self, idx):
                path, y = self.samples[idx]
                try: img = Image.open(path).convert("RGB")
                except: img = Image.new("RGB", (224,224), "black")
                if self.transform: img = self.transform(img)
                return img, y

        def evaluate_model(model, head, loader, device):
            model.eval(); head.eval()
            correct = total = 0
            with torch.no_grad():
                for x, y in loader:
                    x, y = x.to(device), y.to(device)
                    feats = model.encode_image(x)
                    feats = feats / feats.norm(dim=-1, keepdim=True)
                    logits = head(feats)
                    _, pred = torch.max(logits, 1)
                    total += y.size(0)
                    correct += (pred == y).sum().item()
            return 100.0*correct/total if total>0 else 0.0

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--train_data"); ap.add_argument("--val_data"); ap.add_argument("--test_data")
            ap.add_argument("--model_config")
            ap.add_argument("--buttons_actions_model"); ap.add_argument("--content_display_model")
            ap.add_argument("--feedback_status_model"); ap.add_argument("--input_forms_model")
            ap.add_argument("--interactive_elements_model"); ap.add_argument("--layout_structure_model")
            ap.add_argument("--navigation_model"); ap.add_argument("--visual_enhancements_model")
            ap.add_argument("--training_results")
            args = ap.parse_args()

            cfg = json.loads(Path(args.model_config).read_text())
            DATA_DIR, BATCH_SIZE, EPOCHS = Path(cfg["dataset_root"]), int(cfg["batch_size"]), int(cfg["epochs"])
            LR_H, LR_B, WD = float(cfg["lr_head"]), float(cfg["lr_backbone"]), float(cfg["weight_decay"])
            CATS, MEAN, STD, CLIP_MODEL = cfg["categories"], cfg["mean"], cfg["std"], cfg["clip_model_name"]

            DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
            comps_map = get_all_components_mapping(DATA_DIR, CATS)
            results = {}

            path_map = {
              "buttons_actions": args.buttons_actions_model,
              "content_display": args.content_display_model,
              "feedback_status": args.feedback_status_model,
              "input_forms": args.input_forms_model,
              "interactive_elements": args.interactive_elements_model,
              "layout_structure": args.layout_structure_model,
              "navigation": args.navigation_model,
              "visual_enhancements": args.visual_enhancements_model
            }

            for cat in CATS:
                out_path = Path(path_map[cat])
                out_path.parent.mkdir(parents=True, exist_ok=True)

                comps = comps_map.get(cat, [])
                if not comps:
                    # write dummy file so Argo always sees an artifact
                    torch.save({"category": cat, "val_acc": 0.0, "warning": "no components found"}, out_path)
                    results[cat] = 0.0
                    continue

                model, _ = clip.load(CLIP_MODEL, device=DEVICE); model.float()
                train_tf = transforms.Compose([
                    transforms.Resize((224,224)), transforms.RandomResizedCrop(224,scale=(0.8,1.0)),
                    transforms.RandomHorizontalFlip(), transforms.ColorJitter(0.2,0.2,0.2,0.1),
                    transforms.ToTensor(), transforms.Normalize(mean=MEAN, std=STD)
                ])
                val_tf = transforms.Compose([
                    transforms.Resize((224,224)), transforms.ToTensor(),
                    transforms.Normalize(mean=MEAN, std=STD)
                ])
                train_ds = CategoryComponentDataset(DATA_DIR, cat, comps, "train", train_tf)
                val_ds = CategoryComponentDataset(DATA_DIR, cat, comps, "val", val_tf)
                train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
                val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)
                feature_dim = model.visual.output_dim
                head = nn.Sequential(nn.Dropout(0.1), nn.Linear(feature_dim,len(comps))).to(DEVICE)
                opt = optim.AdamW([
                    {"params": head.parameters(),"lr":LR_H},
                    {"params": model.visual.parameters(),"lr":LR_B}], weight_decay=WD)
                crit = nn.CrossEntropyLoss()
                best=0.0
                for ep in range(1,EPOCHS+1):
                    model.train(); head.train()
                    for x,y in train_loader:
                        x,y=x.to(DEVICE),y.to(DEVICE); opt.zero_grad()
                        feats=model.encode_image(x); feats=feats/feats.norm(dim=-1,keepdim=True)
                        logits=head(feats); loss=crit(logits,y); loss.backward(); opt.step()
                    val_acc=evaluate_model(model,head,val_loader,DEVICE)
                    if val_acc>best:
                        best=val_acc
                        save_dict={
                            "clip_model_state":model.state_dict(),
                            "head_state":head.state_dict(),
                            "component_to_idx":train_ds.component_to_idx,
                            "category":cat,
                            "val_acc":val_acc
                        }
                        torch.save(save_dict, out_path)
                results[cat]=best

            Path(args.training_results).parent.mkdir(parents=True, exist_ok=True)
            Path(args.training_results).write_text(json.dumps(results,indent=2))

        if __name__=="__main__": main()

    args:
      - --train_data
      - {inputPath: train_data}
      - --val_data
      - {inputPath: val_data}
      - --test_data
      - {inputPath: test_data}
      - --model_config
      - {inputPath: model_config}
      - --buttons_actions_model
      - {outputPath: buttons_actions_model}
      - --content_display_model
      - {outputPath: content_display_model}
      - --feedback_status_model
      - {outputPath: feedback_status_model}
      - --input_forms_model
      - {outputPath: input_forms_model}
      - --interactive_elements_model
      - {outputPath: interactive_elements_model}
      - --layout_structure_model
      - {outputPath: layout_structure_model}
      - --navigation_model
      - {outputPath: navigation_model}
      - --visual_enhancements_model
      - {outputPath: visual_enhancements_model}
      - --training_results
      - {outputPath: training_results}
