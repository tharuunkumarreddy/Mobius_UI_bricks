name: phase1_train
description: Train the Phase 1 ResNet50 classifier using the provided dataset and model initialization.

inputs:
  - { name: train_data, type: Dataset, description: "Training dataset" }
  - { name: val_data, type: Dataset, description: "Validation dataset" }
  - { name: test_data, type: Dataset, description: "Test dataset" }
  - { name: model_init, type: Model, description: "Model initialization weights (phase1_model_init.pt)" }
  - { name: model_meta, type: String, description: "Model metadata JSON (phase1_model_meta.json)" }

outputs:
  - { name: trained_model, type: Model, description: "Best trained model checkpoint (phase1_resnet50_best.pt)" }
  - { name: label_maps, type: String, description: "Label mappings JSON (phase1_label_maps.json)" }
  - { name: train_log, type: String, description: "Training log JSON (phase1_train_log.json)" }

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        # Update package manager and install system dependencies
        apt-get update && apt-get install -y build-essential
        
        # Install Python packages with proper error handling
        pip install --upgrade pip
        pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cpu
        pip install --no-cache-dir pillow numpy scikit-learn
        
        # Verify installations
        python3 -c "import torch, torchvision, numpy, sklearn; print('All packages installed successfully')"
        
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        #!/usr/bin/env python3
        import argparse
        import json
        import math
        import random
        import time
        import os
        import pickle
        from pathlib import Path
        import numpy as np
        import torch
        import torch.nn as nn
        import torch.backends.cudnn as cudnn
        from torch.utils.data import Dataset, DataLoader
        from torchvision import transforms
        from torchvision.models import resnet50, ResNet50_Weights
        from PIL import Image

        IMG_EXTS = {".jpg", ".jpeg", ".png", ".webp"}

        def set_seed(seed=42):
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            cudnn.deterministic = False
            cudnn.benchmark = True

        def list_images_under_category(category_dir: Path):
            files = []
            print(f"Searching for images in category dir: {category_dir}")
            
            if not category_dir.exists():
                print(f"Category directory does not exist: {category_dir}")
                return files
                
            # Check if images are directly in the category directory
            for f in category_dir.iterdir():
                if f.is_file() and f.suffix.lower() in IMG_EXTS:
                    files.append(f)
                    
            # Also check subdirectories (original logic)
            for comp_dir in category_dir.iterdir():
                if comp_dir.is_dir():
                    for f in comp_dir.iterdir():
                        if f.is_file() and f.suffix.lower() in IMG_EXTS:
                            files.append(f)
                            
            print(f"Found {len(files)} images in {category_dir}")
            return files

        class UICategoryDataset(Dataset):
            def __init__(self, split_root: Path, category_to_idx: dict, transform=None):
                self.transform = transform
                self.samples = []
                self.category_to_idx = category_to_idx
                
                print(f"Initializing dataset from: {split_root}")
                print(f"Split root exists: {split_root.exists()}")
                
                if split_root.exists():
                    print(f"Contents of {split_root}:")
                    for item in split_root.iterdir():
                        print(f"  {item} ({'dir' if item.is_dir() else 'file'})")
                
                for cat in sorted(category_to_idx.keys()):
                    cat_dir = split_root / cat
                    print(f"Looking for category '{cat}' in: {cat_dir}")
                    if not cat_dir.exists(): 
                        print(f"Category directory '{cat}' not found, skipping...")
                        continue
                    images = list_images_under_category(cat_dir)
                    for img_path in images:
                        self.samples.append((img_path, category_to_idx[cat]))
                    print(f"Added {len(images)} images for category '{cat}'")
                
                print(f"Total samples found: {len(self.samples)}")
                if len(self.samples) == 0:
                    # Let's explore the directory structure to understand the issue
                    print("\nDebugging directory structure:")
                    def explore_dir(path, max_depth=3, current_depth=0):
                        if current_depth >= max_depth:
                            return
                        try:
                            for item in path.iterdir():
                                indent = "  " * current_depth
                                if item.is_dir():
                                    print(f"{indent}{item.name}/")
                                    explore_dir(item, max_depth, current_depth + 1)
                                else:
                                    print(f"{indent}{item.name}")
                        except PermissionError:
                            print(f"{indent}[Permission Denied]")
                    
                    explore_dir(split_root)
                    raise RuntimeError(f"No images found under {split_root}. Check directory structure above.")

            def __len__(self):
                return len(self.samples)

            def __getitem__(self, idx):
                path, y = self.samples[idx]
                img = Image.open(path).convert("RGB")
                if self.transform:
                    img = self.transform(img)
                return img, y

        def accuracy(outputs, targets):
            preds = outputs.argmax(dim=1)
            return (preds == targets).float().mean().item()

        def evaluate(model, loader, device, criterion):
            model.eval()
            running_loss, running_acc, n = 0.0, 0.0, 0
            with torch.no_grad():
                for x, y in loader:
                    x, y = x.to(device), y.to(device)
                    with torch.cuda.amp.autocast():
                        logits = model(x)
                        loss = criterion(logits, y)
                    bs = y.size(0)
                    running_loss += loss.item() * bs
                    running_acc  += accuracy(logits, y) * bs
                    n += bs
            return running_loss / n, running_acc / n

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--train_data', type=str, required=True)
            parser.add_argument('--val_data', type=str, required=True)
            parser.add_argument('--test_data', type=str, required=True)
            parser.add_argument('--model_init', type=str, required=True)
            parser.add_argument('--model_meta', type=str, required=True)
            parser.add_argument('--trained_model', type=str, required=True)
            parser.add_argument('--label_maps', type=str, required=True)
            parser.add_argument('--train_log', type=str, required=True)
            parser.add_argument('--epochs', type=int, default=10)
            parser.add_argument('--batch_size', type=int, default=32)
            parser.add_argument('--lr', type=float, default=3e-4)
            parser.add_argument('--weight_decay', type=float, default=1e-4)
            parser.add_argument('--num_workers', type=int, default=2)
            parser.add_argument('--seed', type=int, default=42)
            args = parser.parse_args()

            set_seed(args.seed)

            # Ensure output directories exist
            os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
            os.makedirs(os.path.dirname(args.label_maps), exist_ok=True)
            os.makedirs(os.path.dirname(args.train_log), exist_ok=True)

            # Load dataset paths from pickle files
            def load_dataset_path(dataset_pickle_path):
                print(f"Loading dataset from: {dataset_pickle_path}")
                try:
                    with open(dataset_pickle_path, 'rb') as f:
                        dataset_info = pickle.load(f)
                    if isinstance(dataset_info, dict) and 'data_path' in dataset_info:
                        path = Path(dataset_info['data_path'])
                        print(f"Extracted data_path: {path}")
                        return path
                    else:
                        path = Path(dataset_pickle_path)
                        print(f"Using pickle path directly: {path}")
                        return path
                except Exception as e:
                    print(f"Error loading dataset pickle {dataset_pickle_path}: {e}")
                    # Fallback: treat as direct path
                    path = Path(dataset_pickle_path)
                    print(f"Fallback to direct path: {path}")
                    return path

            train_path = load_dataset_path(args.train_data)
            val_path = load_dataset_path(args.val_data)
            test_path = load_dataset_path(args.test_data)
            
            print(f"Train path: {train_path}")
            print(f"Val path: {val_path}")
            print(f"Test path: {test_path}")
            print(f"Model meta file: {args.model_meta}")
            print(f"Model init file: {args.model_init}")

            # Load model metadata - handle both direct JSON and pickle file
            try:
                # First try to load as JSON
                with open(args.model_meta, 'r') as f:
                    meta = json.load(f)
                print("Loaded metadata as JSON")
            except (json.JSONDecodeError, UnicodeDecodeError):
                # If JSON fails, try pickle
                try:
                    with open(args.model_meta, 'rb') as f:
                        meta = pickle.load(f)
                    print("Loaded metadata as pickle")
                except Exception as e:
                    print(f"Error loading model meta as both JSON and pickle: {e}")
                    print(f"File path: {args.model_meta}")
                    # Try to read first few bytes to debug
                    try:
                        with open(args.model_meta, 'rb') as f:
                            first_bytes = f.read(50)
                            print(f"First 50 bytes: {first_bytes}")
                    except Exception:
                        pass
                    exit(1)
            except Exception as e:
                print(f"Error loading model meta: {e}")
                exit(1)

            image_size = meta["image_size"]
            mean, std = meta["mean"], meta["std"]
            categories = meta["categories"]
            category_to_idx = {c:i for i,c in enumerate(categories)}
            idx_to_category = {i:c for c,i in category_to_idx.items()}

            # Fix: Define dataset_root_path based on train_path
            # Assume the structure is: dataset_root/train, dataset_root/val, dataset_root/test
            dataset_root_path = train_path.parent if train_path.name == "train" else train_path

            print(f"Dataset root: {dataset_root_path}")
            print(f"Categories: {categories}")
            print(f"Number of classes: {len(categories)}")

            train_tfms = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.15, hue=0.02),
                transforms.ToTensor(),
                transforms.Normalize(mean=mean, std=std),
            ])
            eval_tfms = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize(mean=mean, std=std),
            ])

            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"Using device: {device}")

            # Build datasets/loaders
            train_ds = UICategoryDataset(train_path, category_to_idx, transform=train_tfms)
            val_ds = UICategoryDataset(val_path, category_to_idx, transform=eval_tfms)
            train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,
                                      num_workers=args.num_workers, pin_memory=True)
            val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False,
                                    num_workers=args.num_workers, pin_memory=True)

            print(f"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}")

            # Load model
            state = torch.load(args.model_init, map_location="cpu")
            # Rebuild the same architecture used in model_def
            model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
            model.fc = nn.Sequential(
                nn.Dropout(p=0.2), 
                nn.Linear(model.fc.in_features, len(categories))
            )
            model.load_state_dict(state, strict=True)
            model.to(device)

            # Loss/optim/sched
            criterion = nn.CrossEntropyLoss(label_smoothing=0.05)
            optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

            total_steps = args.epochs * (len(train_loader) if len(train_loader) > 0 else 1)
            warmup_steps = max(100, int(0.05 * total_steps))
            
            def lr_lambda(step):
                if step < warmup_steps:
                    return float(step) / float(max(1, warmup_steps))
                progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))
                return 0.5 * (1.0 + math.cos(math.pi * progress))
            
            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
            scaler = torch.cuda.amp.GradScaler()

            # Training loop
            best_val_acc = 0.0
            train_log_data = []
            print("Starting training...")
            
            for epoch in range(1, args.epochs + 1):
                model.train()
                epoch_loss = 0.0
                epoch_acc = 0.0
                seen = 0
                t0 = time.time()
                
                for x, y in train_loader:
                    x, y = x.to(device), y.to(device)
                    optimizer.zero_grad(set_to_none=True)
                    with torch.cuda.amp.autocast():
                        logits = model(x)
                        loss = criterion(logits, y)
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                    scheduler.step()
                    bs = y.size(0)
                    epoch_loss += loss.item() * bs
                    epoch_acc  += accuracy(logits, y) * bs
                    seen += bs
                    
                train_loss = epoch_loss / max(1, seen)
                train_acc  = epoch_acc  / max(1, seen)
                val_loss, val_acc = evaluate(model, val_loader, device, criterion)
                dt = time.time() - t0
                
                # Log epoch data
                epoch_data = {
                    "epoch": epoch,
                    "train_loss": train_loss,
                    "train_acc": train_acc,
                    "val_loss": val_loss,
                    "val_acc": val_acc,
                    "duration": dt
                }
                train_log_data.append(epoch_data)
                
                print(f"Epoch {epoch:02d}/{args.epochs} | {dt:.1f}s | train loss {train_loss:.4f} acc {train_acc:.3f} | val loss {val_loss:.4f} acc {val_acc:.3f}")

                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    torch.save({
                        "model_state": model.state_dict(),
                        "category_to_idx": category_to_idx,
                        "idx_to_category": idx_to_category,
                        "image_size": image_size,
                    }, args.trained_model)
                    print(f"âœ“ Saved new best model (val_acc={val_acc:.3f})")

            # Save label maps
            with open(args.label_maps, 'w') as f:
                json.dump({
                    "category_to_idx": category_to_idx,
                    "idx_to_category": idx_to_category
                }, f, indent=2)

            # Save training log
            with open(args.train_log, 'w') as f:
                json.dump({
                    "best_val_acc": best_val_acc,
                    "epochs": train_log_data,
                    "total_epochs": args.epochs
                }, f, indent=2)

            print(f"Training completed. Best val acc: {best_val_acc:.3f}")
            print(f"Trained model saved to: {args.trained_model}")
            print(f"Label maps saved to: {args.label_maps}")
            print(f"Training log saved to: {args.train_log}")

        if __name__ == "__main__":
            main()

    args:
      - --train_data
      - {inputPath: train_data}
      - --val_data
      - {inputPath: val_data}
      - --test_data
      - {inputPath: test_data}
      - --model_init
      - {inputPath: model_init}
      - --model_meta
      - {inputPath: model_meta}
      - --trained_model
      - {outputPath: trained_model}
      - --label_maps
      - {outputPath: label_maps}
      - --train_log
      - {outputPath: train_log}
