name: phase1_train_model
description: Trains a ResNet50 model using the provided datasets and initial model weights.
inputs:
  - {name: train_data, type: Dataset, description: "Pickled train dataset metadata from the data loading component"}
  - {name: val_data, type: Dataset, description: "Pickled validation dataset metadata from the data loading component"}
  - {name: model_init, type: Model, description: "Path to the initial model weights (.pt file)"}
  - {name: model_meta, type: String, description: "Path to the model metadata (.json file)"}
outputs:
  - {name: best_model, type: Model, description: "The best trained model weights saved in .pt format"}
  - {name: label_maps, type: Dataset, description: "JSON file mapping class indices to class names"}
  - {name: train_log, type: Dataset, description: "JSON file containing training and validation logs"}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        # Install necessary Python libraries, specifically PyTorch for CPU
        pip install torch torchvision pandas scikit-learn --index-url https://download.pytorch.org/whl/cpu
        
        # Execute the main Python script
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import pickle
        import time
        from pathlib import Path

        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torchvision import transforms
        from torchvision.datasets import ImageFolder
        from torch.utils.data import DataLoader
        from torchvision.models import resnet50

        def main():
            # 1. --- Argument Parsing ---
            # This section correctly defines all the command-line arguments
            # that will receive the paths to input and output artifacts from Kubeflow.
            parser = argparse.ArgumentParser(description="PyTorch Model Training")
            parser.add_argument('--train_data', type=str, required=True, help='Path to the pickled training data info')
            parser.add_argument('--val_data', type=str, required=True, help='Path to the pickled validation data info')
            parser.add_argument('--model_init', type=str, required=True, help='Path to initial model weights (.pt)')
            parser.add_argument('--model_meta', type=str, required=True, help='Path to model metadata (.json)')
            parser.add_argument('--best_model', type=str, required=True, help='Path to save the best model weights')
            parser.add_argument('--label_maps', type=str, required=True, help='Path to save the label maps JSON')
            parser.add_argument('--train_log', type=str, required=True, help='Path to save the training log JSON')
            
            # --- Hyperparameters ---
            parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')
            parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')
            parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate for the optimizer')
            args = parser.parse_args()

            # 2. --- Load Inputs ---
            # Load the metadata and initial model from the paths provided by the arguments.
            print("Loading inputs...")
            with open(args.train_data, 'rb') as f:
                train_info = pickle.load(f)
            with open(args.val_data, 'rb') as f:
                val_info = pickle.load(f)
            with open(args.model_meta, 'r') as f:
                meta = json.load(f)

            train_dir = Path(train_info['data_path'])
            val_dir = Path(val_info['data_path'])
            
            num_classes = meta['num_classes']
            image_size = meta['image_size']
            mean = meta['mean']
            std = meta['std']
            categories = meta['categories']
            
            print(f"Number of classes: {num_classes}")
            print(f"Training directory: {train_dir}")
            print(f"Validation directory: {val_dir}")

            # 3. --- Prepare Datasets and DataLoaders ---
            # Define image transformations using the mean and std from the model_meta file.
            data_transforms = {
                'train': transforms.Compose([
                    transforms.RandomResizedCrop(image_size),
                    transforms.RandomHorizontalFlip(),
                    transforms.ToTensor(),
                    transforms.Normalize(mean, std)
                ]),
                'val': transforms.Compose([
                    transforms.Resize(image_size + 32),
                    transforms.CenterCrop(image_size),
                    transforms.ToTensor(),
                    transforms.Normalize(mean, std)
                ]),
            }

            # Create ImageFolder datasets and DataLoaders
            image_datasets = {
                'train': ImageFolder(train_dir, data_transforms['train']),
                'val': ImageFolder(val_dir, data_transforms['val'])
            }
            dataloaders = {x: DataLoader(image_datasets[x], batch_size=args.batch_size, shuffle=True, num_workers=2) for x in ['train', 'val']}
            dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
            class_names = image_datasets['train'].classes
            
            # Verify class names match
            assert class_names == categories, "Class name mismatch between dataset and metadata!"
            print(f"Discovered classes: {class_names}")

            # 4. --- Initialize Model, Optimizer, and Loss Function ---
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            print(f"Using device: {device}")

            # Re-create the exact same model structure as in phase1_model_def
            model = resnet50(weights=None) # No pretrained weights, we will load our own
            model.fc = nn.Sequential(
                nn.Dropout(p=0.2),
                nn.Linear(model.fc.in_features, num_classes)
            )
            
            # Load the initial weights from the previous component
            model.load_state_dict(torch.load(args.model_init, map_location=device))
            model = model.to(device)

            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)

            # 5. --- Training Loop ---
            since = time.time()
            best_model_wts = model.state_dict()
            best_acc = 0.0
            train_log_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

            for epoch in range(args.epochs):
                print(f'Epoch {epoch}/{args.epochs - 1}')
                print('-' * 10)

                for phase in ['train', 'val']:
                    if phase == 'train':
                        model.train()
                    else:
                        model.eval()

                    running_loss = 0.0
                    running_corrects = 0

                    for inputs, labels in dataloaders[phase]:
                        inputs = inputs.to(device)
                        labels = labels.to(device)

                        optimizer.zero_grad()

                        with torch.set_grad_enabled(phase == 'train'):
                            outputs = model(inputs)
                            _, preds = torch.max(outputs, 1)
                            loss = criterion(outputs, labels)

                            if phase == 'train':
                                loss.backward()
                                optimizer.step()

                        running_loss += loss.item() * inputs.size(0)
                        running_corrects += torch.sum(preds == labels.data)

                    epoch_loss = running_loss / dataset_sizes[phase]
                    epoch_acc = running_corrects.double() / dataset_sizes[phase]
                    
                    train_log_history[f'{phase}_loss'].append(epoch_loss)
                    train_log_history[f'{phase}_acc'].append(epoch_acc.item())

                    print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

                    if phase == 'val' and epoch_acc > best_acc:
                        best_acc = epoch_acc
                        best_model_wts = model.state_dict()
                        print(f"New best validation accuracy: {best_acc:.4f}, saving model.")
                        # Save the best model weights to the path specified by Kubeflow
                        torch.save(best_model_wts, args.best_model)


            time_elapsed = time.time() - since
            print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
            print(f'Best val Acc: {best_acc:4f}')

            # 6. --- Save Outputs ---
            print("Saving final artifacts...")
            
            # The best model is already saved in the loop. This is just a confirmation.
            print(f"Best model saved to {args.best_model}")

            # Create and save label maps
            label_map_dict = {i: name for i, name in enumerate(class_names)}
            with open(args.label_maps, 'w') as f:
                json.dump(label_map_dict, f, indent=2)
            print(f"Label maps saved to {args.label_maps}")

            # Save training log
            with open(args.train_log, 'w') as f:
                json.dump(train_log_history, f, indent=2)
            print(f"Training log saved to {args.train_log}")

        if __name__ == '__main__':
            main()
    args:
      - --train_data
      - {inputPath: train_data}
      - --val_data
      - {inputPath: val_data}
      - --model_init
      - {inputPath: model_init}
      - --model_meta
      - {inputPath: model_meta}
      - --best_model
      - {outputPath: best_model}
      - --label_maps
      - {outputPath: label_maps}
      - --train_log
      - {outputPath: train_log}
