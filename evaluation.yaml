name: evaluation
description: >
  Evaluate trained models on test split.
  - Phase 1: report loss/accuracy on test split using the best ResNet50 checkpoint.
  - Phase 2: evaluate per-category CLIP models on test split.
  Outputs phase1_test_metrics.json and phase2_eval_results.json.

inputs:
  - { name: train_data,   type: Dataset, description: "Directory: train split (category/component/IMGs)" }
  - { name: val_data,     type: Dataset, description: "Directory: val split (category/component/IMGs)" }
  - { name: test_data,    type: Dataset, description: "Directory: test split (category/component/IMGs)" }
  - { name: phase1_model, type: Model,   description: "Best Phase 1 ResNet50 checkpoint (.pt)" }
  - { name: model_meta,   type: Dataset, description: "Phase 1 model metadata JSON (contains image_size, mean, std, categories)" }
  - { name: label_maps,   type: Dataset, description: "JSON mapping: {index: category}" }
  - { name: buttons_actions_model,      type: Model, description: "Best CLIP model for buttons_actions category" }
  - { name: content_display_model,      type: Model, description: "Best CLIP model for content_display category" }
  - { name: feedback_status_model,      type: Model, description: "Best CLIP model for feedback_status category" }
  - { name: input_forms_model,          type: Model, description: "Best CLIP model for input_forms category" }
  - { name: interactive_elements_model, type: Model, description: "Best CLIP model for interactive_elements category" }
  - { name: layout_structure_model,     type: Model, description: "Best CLIP model for layout_structure category" }
  - { name: navigation_model,           type: Model, description: "Best CLIP model for navigation category" }
  - { name: visual_enhancements_model,  type: Model, description: "Best CLIP model for visual_enhancements category" }

outputs:
  - { name: phase1_test_metrics, type: Dataset, description: "JSON with Phase 1 test loss/accuracy (phase1_test_metrics.json)" }
  - { name: phase2_eval_results, type: Dataset, description: "JSON with Phase 2 per-category test accuracies (phase2_eval_results.json)" }

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        set -e
        pip install torch torchvision pillow --index-url https://download.pytorch.org/whl/cpu
        pip install git+https://github.com/openai/CLIP.git
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, json, statistics
        from pathlib import Path
        import torch, torch.nn as nn
        from torch.utils.data import Dataset, DataLoader
        from torchvision import transforms
        from PIL import Image
        import clip

        IMG_EXTS = {".jpg",".jpeg",".png",".webp"}

        # -------- Phase1 --------
        def list_images_under_category(category_dir: Path):
            files=[]
            for comp_dir in category_dir.iterdir():
                if comp_dir.is_dir():
                    for f in comp_dir.iterdir():
                        if f.is_file() and f.suffix.lower() in IMG_EXTS:
                            files.append(f)
            return files

        class UICategoryDataset(Dataset):
            def __init__(self, split_root: Path, category_to_idx: dict, transform=None):
                self.transform=transform; self.samples=[]
                for cat, idx in category_to_idx.items():
                    cat_dir=split_root/cat
                    if not cat_dir.exists(): continue
                    for img_path in list_images_under_category(cat_dir):
                        self.samples.append((img_path, idx))
            def __len__(self): return len(self.samples)
            def __getitem__(self, idx):
                path,y=self.samples[idx]
                try: img=Image.open(path).convert("RGB")
                except: img=Image.new("RGB",(224,224),"black")
                if self.transform: img=self.transform(img)
                return img,y

        def eval_phase1(test_dir: Path, ckpt_path: Path, meta_path: Path, labels_path: Path, out_json: Path):
            out_json.parent.mkdir(parents=True, exist_ok=True)

            if not ckpt_path.exists() or not test_dir.exists():
                print("[Phase1] Missing checkpoint or test dir; skipping.")
                out_json.write_text(json.dumps({"note":"no test data or checkpoint"}, indent=2))
                return

            meta = json.loads(Path(meta_path).read_text())
            image_size = int(meta["image_size"])
            categories = meta["categories"]
            category_to_idx = {c: i for i, c in enumerate(categories)}
            idx_to_category = {i: c for c, i in category_to_idx.items()}

            if not any((test_dir/c).exists() and list((test_dir/c).glob("*")) for c in categories):
                print(f"[Phase1] No test images under {test_dir}, skipping.")
                out_json.write_text(json.dumps({"note":"no test samples"}, indent=2))
                return

            from torchvision.models import resnet50, ResNet50_Weights
            m = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
            m.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(m.fc.in_features,len(categories)))
            state_dict = torch.load(ckpt_path,map_location="cpu")
            m.load_state_dict(state_dict, strict=True)

            device=torch.device("cuda" if torch.cuda.is_available() else "cpu"); m.to(device)
            weights=ResNet50_Weights.IMAGENET1K_V2
            eval_tfms=transforms.Compose([transforms.Resize((image_size,image_size)),transforms.ToTensor(),
                                          transforms.Normalize(mean=weights.transforms().mean,std=weights.transforms().std)])
            test_ds=UICategoryDataset(test_dir,category_to_idx,transform=eval_tfms)
            if len(test_ds)==0:
                print(f"[Phase1] No valid test samples after scanning {test_dir}.")
                out_json.write_text(json.dumps({"note":"no test samples"}, indent=2))
                return

            test_loader=DataLoader(test_ds,batch_size=32,shuffle=False,num_workers=2,pin_memory=True)
            crit=nn.CrossEntropyLoss(); m.eval(); tot_loss=tot_acc=n=0
            with torch.no_grad():
                for x,y in test_loader:
                    x,y=x.to(device),y.to(device)
                    logits=m(x); loss=crit(logits,y)
                    bs=y.size(0); tot_loss+=loss.item()*bs; tot_acc+=(logits.argmax(1)==y).sum().item(); n+=bs
            metrics={"loss":tot_loss/n,"acc":tot_acc/n,"num_examples":n,
                     "num_classes":len(categories),"labels":idx_to_category}
            out_json.write_text(json.dumps(metrics,indent=2))
            print("[Phase1] Test metrics:",metrics)

        # -------- Phase2 --------
        class CategoryComponentDataset(Dataset):
            def __init__(self,root:Path,category:str,components,split="test",transform=None):
                self.transform=transform; self.samples=[]; self.component_to_idx={c:i for i,c in enumerate(components)}
                cat_dir=root/split/category
                if not cat_dir.exists():
                    return
                for comp in components:
                    comp_dir=cat_dir/comp
                    if comp_dir.exists():
                        for f in comp_dir.iterdir():
                            if f.suffix.lower() in IMG_EXTS:
                                self.samples.append((f,self.component_to_idx[comp]))
            def __len__(self): return len(self.samples)
            def __getitem__(self,idx):
                p,y=self.samples[idx]
                try: img=Image.open(p).convert("RGB")
                except: img=Image.new("RGB",(224,224),"black")
                if self.transform: img=self.transform(img)
                return img,y

        def eval_phase2(test_root:Path, model_files, out_json:Path):
            out_json.parent.mkdir(parents=True, exist_ok=True)

            MEAN=[0.48145466,0.4578275,0.40821073]; STD=[0.26862954,0.26130258,0.27577711]
            tfm=transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor(),transforms.Normalize(mean=MEAN,std=STD)])
            device="cuda" if torch.cuda.is_available() else "cpu"; results={}
            val_accs=[]; test_accs=[]
            for pt in model_files:
                ckpt=torch.load(pt,map_location="cpu")
                category=ckpt["category"]; components=ckpt["components"]; val_acc=ckpt.get("val_acc",None)
                model,_=clip.load("ViT-B/32",device=device); model.float()
                head=nn.Sequential(nn.Dropout(0.1), nn.Linear(model.visual.output_dim,len(components))).to(device)
                model.load_state_dict(ckpt["clip_model_state"],strict=True)
                head.load_state_dict(ckpt["head_state"],strict=True)
                try:
                    ds=CategoryComponentDataset(test_root,category,components,split="test",transform=tfm)
                    if len(ds)==0:
                        print(f"[Phase2] Skipping {category}: no test samples found")
                        results[category]={"val_acc":val_acc,"test_acc":None,"num_examples":0,"num_components":len(components),"note":"no test samples"}
                        if val_acc is not None: val_accs.append(val_acc)
                        continue
                    dl=DataLoader(ds,batch_size=16,shuffle=False,num_workers=2,pin_memory=True)
                    correct=total=0; model.eval(); head.eval()
                    with torch.no_grad():
                        for x,y in dl:
                            x,y=x.to(device),y.to(device)
                            feats=model.encode_image(x); feats=feats/feats.norm(dim=-1,keepdim=True)
                            logits=head(feats); _,pred=torch.max(logits,1)
                            total+=y.size(0); correct+=(pred==y).sum().item()
                    test_acc=100.0*correct/total if total else 0.0
                    results[category]={"val_acc":val_acc,"test_acc":test_acc,"num_examples":total,"num_components":len(components)}
                    print(f"[Phase2] {category}: test_acc={test_acc:.2f}% ({correct}/{total}), val_acc={val_acc}")
                    if val_acc is not None: val_accs.append(val_acc)
                    test_accs.append(test_acc)
                except Exception as e:
                    results[category]={"val_acc":val_acc,"test_acc":0.0,"num_examples":0,"num_components":len(components),"error":str(e)}

            summary={
                "avg_val_acc": statistics.mean(val_accs) if val_accs else None,
                "avg_test_acc": statistics.mean(test_accs) if test_accs else None,
                "num_categories": len(model_files),
                "num_with_test": len(test_accs)
            }
            results["_summary"]=summary
            out_json.write_text(json.dumps(results,indent=2))
            print("[Phase2] Results with summary:",results)

        def main():
            ap=argparse.ArgumentParser()
            ap.add_argument("--train_data"); ap.add_argument("--val_data"); ap.add_argument("--test_data")
            ap.add_argument("--phase1_model"); ap.add_argument("--model_meta"); ap.add_argument("--label_maps")
            ap.add_argument("--buttons_actions_model"); ap.add_argument("--content_display_model")
            ap.add_argument("--feedback_status_model"); ap.add_argument("--input_forms_model")
            ap.add_argument("--interactive_elements_model"); ap.add_argument("--layout_structure_model")
            ap.add_argument("--navigation_model"); ap.add_argument("--visual_enhancements_model")
            ap.add_argument("--phase1_test_metrics"); ap.add_argument("--phase2_eval_results")
            args=ap.parse_args()

            test_root=Path(args.test_data)

            eval_phase1(test_root, Path(args.phase1_model), Path(args.model_meta), Path(args.label_maps), Path(args.phase1_test_metrics))

            model_files=[Path(args.buttons_actions_model),Path(args.content_display_model),
                         Path(args.feedback_status_model),Path(args.input_forms_model),
                         Path(args.interactive_elements_model),Path(args.layout_structure_model),
                         Path(args.navigation_model),Path(args.visual_enhancements_model)]
            eval_phase2(test_root, model_files, Path(args.phase2_eval_results))

        if __name__=="__main__": main()

    args:
      - --train_data
      - {inputPath: train_data}
      - --val_data
      - {inputPath: val_data}
      - --test_data
      - {inputPath: test_data}
      - --phase1_model
      - {inputPath: phase1_model}
      - --model_meta
      - {inputPath: model_meta}
      - --label_maps
      - {inputPath: label_maps}
      - --buttons_actions_model
      - {inputPath: buttons_actions_model}
      - --content_display_model
      - {inputPath: content_display_model}
      - --feedback_status_model
      - {inputPath: feedback_status_model}
      - --input_forms_model
      - {inputPath: input_forms_model}
      - --interactive_elements_model
      - {inputPath: interactive_elements_model}
      - --layout_structure_model
      - {inputPath: layout_structure_model}
      - --navigation_model
      - {inputPath: navigation_model}
      - --visual_enhancements_model
      - {inputPath: visual_enhancements_model}
      - --phase1_test_metrics
      - {outputPath: phase1_test_metrics}
      - --phase2_eval_results
      - {outputPath: phase2_eval_results}
