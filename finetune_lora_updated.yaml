name: FineTune Gemma3 LoRA
description: Fine-tunes a pretrained Gemma3 model using LoRA while preserving training schema and loss tracking.
inputs:
  - {name: tokenizer_json,            type: String, description: "Path to tokenizer.json"}
  - {name: train_corpus,              type: String, description: "Path to training text file"}
  - {name: model_config,              type: String, description: "Path to model config JSON"}
  - {name: pretrained_weights,        type: String, description: "Path to pretrained model .pt/.bin"}
  - {name: model_py_in,               type: String, description: "Path to Gemma3 model .py file"}
  - {name: learning_rate,             type: Float,  default: 1e-4}
  - {name: min_lr,                    type: Float,  default: 1e-5}
  - {name: warmup_steps,              type: Integer, default: 1000}
  - {name: max_iters,                 type: Integer, default: 150000}
  - {name: batch_size,                type: Integer, default: 32}
  - {name: block_size,                type: Integer, default: 128}
  - {name: grad_accum,                type: Integer, default: 32}
  - {name: eval_interval,             type: Integer, default: 1000}
  - {name: eval_iters,                type: Integer, default: 500}
  - {name: weight_decay,              type: Float,  default: 0.1}
  - {name: beta2,                     type: Float,  default: 0.95}
  - {name: clip_grad_norm,            type: Float,  default: 0.5}
  - {name: val_fraction,              type: Float,  default: 0.1}
  - {name: num_proc,                  type: Integer, default: 8}
  - {name: lora_r,                    type: Integer, default: 8}
  - {name: lora_alpha,                type: Integer, default: 8}
  - {name: lora_dropout,              type: Float,  default: 0.1}
  - {name: lora_bias,                 type: String, default: "none"}
outputs:
  - {name: best_weights,     type: String, description: "Path to best LoRA-adapted weights"}
  - {name: final_weights,    type: String, description: "Path to final LoRA-adapted weights"}
  - {name: training_report,  type: String, description: "JSON report with best loss/updates"}
  - {name: loss_curve_csv,   type: String, description: "CSV of train/val loss and lr per eval"}
  - {name: model_py,         type: String, description: "Copied model.py (for lineage)"}
  - {name: schema_json,      type: String, description: "Schema-like summary (loss timeline)"}
implementation:
  container:
    image: python:3.9
    command:
      - python3
      - -u
      - -c
      - |-
        import subprocess, sys
        subprocess.run([sys.executable, "-m", "pip", "install", "--quiet", "--no-cache-dir",
                        "torch>=2.2",
                        "transformers>=4.41",
                        "datasets>=2.19",
                        "tqdm",
                        "peft>=0.11.0",
                        "numpy",
                        "tokenizers>=0.15",
                        "accelerate>=0.29"], check=True)

        import os, json, argparse, shutil, math
        import torch
        import torch.nn.functional as F
        import numpy as np
        from contextlib import nullcontext
        from tokenizers import Tokenizer
        from datasets import load_dataset
        from tqdm import tqdm
        from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training
        from importlib.util import spec_from_file_location, module_from_spec
        import csv

        def choose_bin_dtype(vocab_size):
            if vocab_size <= (1 << 16): return np.uint16
            if vocab_size <= (1 << 32): return np.uint32
            return np.uint64

        def tokenize_to_bins(tokenizer_path, train_txt, val_fraction, num_proc):
            tok = Tokenizer.from_file(tokenizer_path)
            vocab_size = tok.get_vocab_size()
            BIN_DTYPE = choose_bin_dtype(vocab_size)
            ds_single = load_dataset("text", data_files={"train": train_txt})
            split = ds_single["train"].train_test_split(test_size=float(val_fraction), seed=42)
            ds = {"train": split["train"], "val": split["test"]}

            def proc(ex):
                ids = tok.encode(ex["text"]).ids
                return {"ids": ids, "len": len(ids)}

            nproc = max(1, min(int(num_proc), (os.cpu_count() or 1)))
            tokenized = {sp: dset.map(proc, remove_columns=dset.column_names, desc=f"tokenizing {sp}", num_proc=nproc) for sp, dset in ds.items()}

            def write_split(dset, filename):
                total = int(np.sum(dset["len"], dtype=np.uint64))
                if total == 0:
                    raise ValueError(f"{filename}: no tokens to write. Check tokenizer/corpus.")
                arr = np.memmap(filename, dtype=BIN_DTYPE, mode="w+", shape=(total,))
                shard_size = max(1, len(dset) // 1024)
                total_shards = max(1, (len(dset) + shard_size - 1) // shard_size)
                idx = 0
                for i in tqdm(range(total_shards), desc=f"writing {filename}"):
                    start, stop = i*shard_size, min(len(dset), (i+1)*shard_size)
                    if start >= stop: continue
                    batch = dset.select(range(start, stop)).with_format(type="numpy")
                    ids_list = batch["ids"]
                    if not ids_list: continue
                    arr_batch = np.concatenate([np.asarray(x, dtype=arr.dtype) for x in ids_list])
                    arr[idx: idx+len(arr_batch)] = arr_batch
                    idx += len(arr_batch)
                arr.flush()
                return total

            train_tokens = write_split(tokenized["train"], "train.bin")
            val_tokens = write_split(tokenized["val"], "val.bin")
            meta = {"vocab_size": vocab_size, "train_tokens": int(train_tokens), "val_tokens": int(val_tokens), "bin_dtype": str(BIN_DTYPE.__name__)}
            with open("bin_meta.json", "w") as f: json.dump(meta,f,indent=2)
            return meta

        def make_get_batch(block_size, batch_size):
            with open("bin_meta.json", "r") as f: meta=json.load(f)
            dtype_map = {"uint16": np.uint16, "uint32": np.uint32, "uint64": np.uint64}
            BIN_DTYPE = dtype_map[meta["bin_dtype"]]
            TRAIN_BIN, VAL_BIN = "train.bin", "val.bin"
            def get_batch(split):
                path = TRAIN_BIN if split=="train" else VAL_BIN
                data = np.memmap(path, dtype=BIN_DTYPE, mode="r")
                if len(data) <= block_size:
                    raise ValueError(f"{path}: not enough tokens for block_size={block_size}")
                ix = torch.randint(len(data)-block_size-1, (batch_size,))
                x = torch.stack([torch.from_numpy(np.asarray(data[i:i+block_size], dtype=np.int64)) for i in ix])
                y = torch.stack([torch.from_numpy(np.asarray(data[i+1:i+block_size+1], dtype=np.int64)) for i in ix])
                return x, y
            return get_batch

        def estimate_loss(model, get_batch, eval_iters, ctx):
            out = {}
            model.eval()
            with torch.inference_mode():
                for split in ["train","val"]:
                    losses = torch.zeros(eval_iters, device="cpu")
                    for k in range(eval_iters):
                        X, Y = get_batch(split)
                        X, Y = X.to(model.device), Y.to(model.device)
                        with ctx:
                            _, loss = model(X,Y)
                        losses[k] = loss.item()
                    out[split] = losses.mean().item()
            model.train()
            return out

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--tokenizer-json", required=True)
            ap.add_argument("--train-corpus", required=True)
            ap.add_argument("--model-config", required=True)
            ap.add_argument("--pretrained-weights", required=True)
            ap.add_argument("--model_py_in", required=True)
            ap.add_argument("--model-py-out", required=True)
            ap.add_argument("--learning-rate", type=float, required=True)
            ap.add_argument("--min-lr", type=float, required=True)
            ap.add_argument("--warmup-steps", type=int, required=True)
            ap.add_argument("--max-iters", type=int, required=True)
            ap.add_argument("--batch-size", type=int, required=True)
            ap.add_argument("--block-size", type=int, required=True)
            ap.add_argument("--grad-accum", type=int, required=True)
            ap.add_argument("--eval-interval", type=int, required=True)
            ap.add_argument("--eval-iters", type=int, required=True)
            ap.add_argument("--weight-decay", type=float, required=True)
            ap.add_argument("--beta2", type=float, required=True)
            ap.add_argument("--clip-grad-norm", type=float, required=True)
            ap.add_argument("--val-fraction", type=float, required=True)
            ap.add_argument("--num-proc", type=int, required=True)
            ap.add_argument("--lora_r", type=int, default=8)
            ap.add_argument("--lora_alpha", type=int, default=8)
            ap.add_argument("--lora_dropout", type=float, default=0.1)
            ap.add_argument("--lora_bias", type=str, default="none")
            ap.add_argument("--best-weights", required=True)
            ap.add_argument("--final-weights", required=True)
            ap.add_argument("--training-report", required=True)
            ap.add_argument("--loss-curve-csv", required=True)
            ap.add_argument("--schema-output", required=True)
            args = ap.parse_args()

            shutil.copyfile(args.model_py_in, args.model_py_out)
            with open(args.model_config,"r") as f: cfg = json.load(f)
            device = "cuda" if torch.cuda.is_available() else "cpu"

            spec_mod = spec_from_file_location("model_module", args.model_py_out)
            mod = module_from_spec(spec_mod)
            spec_mod.loader.exec_module(mod)
            Gemma3Model = getattr(mod, "Gemma3Model")
            state = torch.load(args.pretrained_weights, map_location=device)
            model = Gemma3Model(cfg).to(device)
            model.load_state_dict(state, strict=True)

            try:
                model = prepare_model_for_int8_training(model)
            except Exception as e:
                # Fallback if int8 prep requires unavailable backends
                pass

            peft_config = LoraConfig(r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout, bias=args.lora_bias)
            model = get_peft_model(model, peft_config)
            model.train()

            _ = tokenize_to_bins(args.tokenizer_json, args.train_corpus, args.val_fraction, args.num_proc)
            get_batch = make_get_batch(args.block_size, args.batch_size)

            optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, betas=(0.9,args.beta2), weight_decay=args.weight_decay)
            from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR
            total_updates = max(1, math.ceil(args.max_iters / max(1,args.grad_accum)))
            warm_updates = min(args.warmup_steps, total_updates)
            sched_warm = LinearLR(optimizer, total_iters=warm_updates)
            sched_cos = CosineAnnealingLR(optimizer, T_max=max(1, total_updates-warm_updates), eta_min=args.min_lr)
            scheduler = SequentialLR(optimizer, schedulers=[sched_warm,sched_cos], milestones=[warm_updates])

            best_val = float("inf")
            train_curve, val_curve, lr_curve = [], [], []
            updates = 0
            ctx = torch.amp.autocast(device_type="cuda", dtype=torch.float16) if device=="cuda" else nullcontext()
            pbar = tqdm(range(args.max_iters), desc="train micro-steps")

            optimizer.zero_grad(set_to_none=True)
            for step in pbar:
                X,Y = get_batch("train")
                X, Y = X.to(device), Y.to(device)
                with ctx:
                    _, loss = model(X,Y)
                (loss/args.grad_accum).backward()
                if ((step+1)%args.grad_accum==0) or (step+1==args.max_iters):
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.clip_grad_norm)
                    optimizer.step()
                    optimizer.zero_grad(set_to_none=True)
                    scheduler.step()
                    updates+=1
                    if (updates % max(1,args.eval_interval)) == 0:
                        losses = estimate_loss(model,get_batch,args.eval_iters,ctx)
                        train_curve.append(losses["train"]); val_curve.append(losses["val"]); lr_curve.append(optimizer.param_groups[0]["lr"])
                        pbar.set_postfix(train=f"{losses['train']:.4f}", val=f"{losses['val']:.4f}", lr=f"{optimizer.param_groups[0]['lr']:.2e}")
                        if losses["val"] < best_val:
                            best_val = losses["val"]
                            torch.save(model.state_dict(), args.best_weights)

            torch.save(model.state_dict(), args.final_weights)
            report = {"best_val_loss": best_val,"updates": updates,"max_iters": args.max_iters}
            with open(args.training_report, "w") as f: json.dump(report,f,indent=2)

            os.makedirs(os.path.dirname(args.loss_curve_csv) or ".",exist_ok=True)
            with open(args.loss_curve_csv,"w",newline="",encoding="utf-8") as f:
                w = csv.writer(f); w.writerow(["update","train_loss","val_loss","lr"])
                for i,(tr,va,lr) in enumerate(zip(train_curve,val_curve,lr_curve),start=1):
                    w.writerow([i,tr,va,lr])

            schema_list = [{"epoch":int(i),"loss":float(tr),"validation_loss":float(va)} for i,(tr,va) in enumerate(zip(train_curve,val_curve),start=1)]
            with open(args.schema_output,"w") as f: json.dump(schema_list,f,separators=(",",":"))

        if __name__=="__main__": main()
    args:
      - --tokenizer-json
      - {inputPath: tokenizer_json}
      - --train-corpus
      - {inputPath: train_corpus}
      - --model-config
      - {inputPath: model_config}
      - --pretrained-weights
      - {inputPath: pretrained_weights}
      - --model_py_in
      - {inputPath: model_py_in}
      - --model-py-out
      - {outputPath: model_py}
      - --learning-rate
      - {inputValue: learning_rate}
      - --min-lr
      - {inputValue: min_lr}
      - --warmup-steps
      - {inputValue: warmup_steps}
      - --max-iters
      - {inputValue: max_iters}
      - --batch-size
      - {inputValue: batch_size}
      - --block-size
      - {inputValue: block_size}
      - --grad-accum
      - {inputValue: grad_accum}
      - --eval-interval
      - {inputValue: eval_interval}
      - --eval-iters
      - {inputValue: eval_iters}
      - --weight-decay
      - {inputValue: weight_decay}
      - --beta2
      - {inputValue: beta2}
      - --clip-grad-norm
      - {inputValue: clip_grad_norm}
      - --val-fraction
      - {inputValue: val_fraction}
      - --num-proc
      - {inputValue: num_proc}
      - --lora_r
      - {inputValue: lora_r}
      - --lora_alpha
      - {inputValue: lora_alpha}
      - --lora_dropout
      - {inputValue: lora_dropout}
      - --lora_bias
      - {inputValue: lora_bias}
      - --best-weights
      - {outputPath: best_weights}
      - --final-weights
      - {outputPath: final_weights}
      - --training-report
      - {outputPath: training_report}
      - --loss-curve-csv
      - {outputPath: loss_curve_csv}
      - --schema-output
      - {outputPath: schema_json}
